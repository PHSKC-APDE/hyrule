---
title: "ML Record Linkage Pipeline"
format: 
  gfm:
    toc: true
    toc-depth: 2
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

# About

This document is an example implementation of machine learning record linkage within a [targets workflow](https://books.ropensci.org/targets/). While it is designed to be a runnable pipeline, the primary goal of this example is to provide ideas and show options that analysts can adapt to their own projects.

The funding and inspiration for this document derives from the NO HARMS grant conducted at Public Health - Seattle & King County and funded by the CDC. The pipeline and methods described below are heavily influenced by the NO HARMS pipeline, but are intended to be more generalized and flexible.

## Using machine learning to do record linkage

### Record linkage (in brief)

Record linkage (aka entity resolution) is the process by which records within and/or between datasets are analyzed and grouped together such that they (ideally) represent a single entity (e.g. a person). [(Almost) all of entity resolution](https://www.science.org/doi/full/10.1126/sciadv.abi8021) and the documentation for the [Splink](https://moj-analytical-services.github.io/splink/topic_guides/topic_guides_index.html) package are excellent resources and provide a more comprehensive description of record linkage and its variations.

### Why use machine learning?

1.  Machine learning (and most regression-esque approaches) are relatively flexible. If you can convert your problem into a series of rows (e.g. whether two records match) with some predictor variables – you can fit your model. Although the flexible interface does occasionally mean it takes longer to get started, users get a lot more runway – especially when they want to be clever and/or create customized predictors. For example, this workflow compares the address histories between two records to determine the minimum observed geographic distance which is used a predictor. This type of thing is difficult to encode in a probabilistic interface.
2.  Ensembling methods (which flow naturally from a regression/machine learning type approach) allow users to employ multiple model families (and their various permutations) to generate the final match score. In the ideal case, each model will be good at a particular part of the overall problem, and then the whole of the models will be greater than the sum of the parts (or at least, better than an individual part). If nothing else,, probabilistic approaches can be used within an overall ensemble.
3.  Any good record linkage project is going to require some manner of verifying if the resulting match determinations are any good (e.g. is A a link to B). Since the "hassle" of creating a manually labeled training dataset already needs to get done – users might as well consider fitting some models on it. In practice, only a few hundred labeled pairs (maybe 2 - 3 hours of work) is required to get a ensemble up and running.

### Useful Concepts

Users should familiarize themselves with the following R-packages and/or methodological concepts:

1.  Analysis pipeline(s): This workflow uses the [targets](https://books.ropensci.org/targets/) and [`tarchetypes`](https://docs.ropensci.org/tarchetypes/) to orchestrate the linkage pipeline. A targets pipeline is a directed acyclic graph that articulates how inputs, intermediate steps, and outputs all interact/flow into each other. Once the pipeline is specified, the targets package uses static code analysis (and other tricks) to determine how to keep the pipeline internally consistent and up to date. Via some partner packages (e.g. [`crew`](https://github.com/wlandau/crew)), a targets pipeline can be computed in parallel fairly easily as the full dependency chain is pre-specified.
2.  Data manipulation and storage: most data tasks (e.g. cleaning, reshaping) are conducted via the [`data.table`](https://github.com/Rdatatable/data.table) R-package or [duckdb](https://github.com/duckdb/duckdb-r) dialect sql (via the `DBI`) package. The [`glue`](https://glue.tidyverse.org/) package is often used to craft sql statements. The [`arrow`](https://arrow.apache.org/docs/r/) package (along with `duckdb`) provides read/write routines for `.parquet` files. `.parquet` files are used because they are reasonably efficient storage wise and allow for partial read/writes.
3.  [stacking/ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning): The linkage method is an ensemble of machine learning models combined via stacked generalization (aka stacking). The [`stacks`](https://stacks.tidymodels.org/) package provides the implementation of the ensembling approach while [`xgboost`](https://xgboost.readthedocs.io/en/stable/R-package/xgboostPresentation.html), [`ranger`](https://github.com/imbs-hl/ranger), and [`kernlab`](https://cran.r-project.org/web/packages/kernlab/kernlab.pdf) implement algorithms that are ensembled together. [Lasso regressions](https://en.wikipedia.org/wiki/Lasso_(statistics)) via [`glmnet`](https://cran.r-project.org/web/packages/glmnet/index.html) are used as a screening model. Overall, the `hyrule` package and the `tidymodels` suite (e.g. [`parnsip`](https://parsnip.tidymodels.org/), [`workflows`](https://workflows.tidymodels.org/), [`tune`](https://tune.tidymodels.org/), [`stacks`](https://stacks.tidymodels.org/), and [`yardstick`](https://yardstick.tidymodels.org/)`)` of packages are used the implement the algorithms
4.  Networks/clusters: Collections of 1:1 matches may aggregate into full-fledged networks and can be analyzed as such. [`igraph`](https://en.wikipedia.org/wiki/Ensemble_learning) is used to analyze and adjust the linkage networks.
5.  [string distance metrics](https://journal.r-project.org/archive/2014-1/loo.pdf): Many of the character variable based comparisons (e.g. name similarity) are string distance metrics. More details can be found here. Implementation is done via [`DuckDB`](https://duckdb.org/docs/sql/functions/char.html) routines.

# Set up

## Pipeline setup

These packages are required to set up the pipeline.

```{r libload}
library(targets)
# library('hyrule')
library(tarchetypes)
library(glue)
# library('sf')
library('data.table')
tar_source()
```

Remove the `_targets_r` directory previously written by non-interactive runs of the report.

```{r tunscript}
tar_unscript()
```

## Packages and functions

These packages and functions are required to have the pipeline run.

```{targets define-globals, tar_globals = TRUE}
tar_option_set(
  packages = c('data.table', 'hyrule', 'stringr', 'arrow', 'duckdb', 'DBI', 'glue', 'stringr', 'sf', 'tidymodels', 'workflows', 'stacks', 'dplyr', 'ranger', 'xgboost', 'rlang', 'igraph')
)

# Much of the code/functions to make the pipeline run are stored in example_workflow/R
tar_source()

# Specify input and output directories
outdir = 'output/'

# other global variables
apply_screen = TRUE
bounds = c(.01, .99)

```

# ML Record Linkage Pipeline

### The "targets" notation

The pipeline specified below uses functions and notation from the `targets` package. Each "target" or "step" follows the general form of `tar_target(output name, operation(inputs))` where "output_name" is the name of the target that is created by conducting an "operation" on a set of "inputs". Sometimes, functions from the `tarchetypes` package are employed. These generally follow the same convention of `tar_target`, but are used to specific types of inputs (e.g. file paths) and/or classes of target generate (e.g. make a new target by some set of parameters).

### Section overview

Each subsequent section will roughly the same structure:

-   Overview: A description of what the series of targets accomplishes

-   Targets: Code and commentary about the individual targets (i.e. steps)

-   General Commentary: Any additional bits of information relevant to the user about the given section

## Data Preparation

### Overview

This series of targets loads and cleans the input data. Once cleaned, some derivative variables are created. The outputs of this section will be used in subsequent sections to create the predictor variables within the model frame(s) in the training and prediction series of targets.

### Targets

#### Specify Input Files

The two datasets (`fake_one.parquet` and `fake_two.parquet`) to be linked are registered by `tar_files_input` and the references are stored into `input_1` and `input_2`.

```{targets specify-input-data}
list(
  # file paths to data
  tarchetypes::tar_files_input(input_1, '../data-raw/fake_one.parquet'),
  tarchetypes::tar_files_input(input_2, '../data-raw/fake_two.parquet'),
)

```

#### Prepare Primary Identifiers

The `data` target (a data frame saved to disk as a parquet file) is created by appending `input_1` and `input_2` together and conducting some data cleaning on primary identifiers like name and date of birth. A new unique row identifier (called `clean_hash`) is also created at this step. A detailed description of what `init_data` does is provided in the "General Commentary" section below.

```{targets clean-primary-ids}
list(
  # Load, clean, and save datasets
  tar_target(data, init_data(
    c(input_1, input_2), 
    file.path(outdir, 'data.parquet')), format = 'file')
)
```

#### Prepare list/"history" variables

`lh` and `zh` are list (i.e. history) variables saved as data frames within parquet files for location and zip code respectively. The `create_location_history` and `create_history_variable` functions compile the unique list of locations (or ZIP code) by `source_system` and `source_id`. The resulting information is used later when constructing list intersection flags for the model frame (e.g. these two records have been observed at the same location).

```{targets 2nd-vars}
list(
  # Prepare the location histories
  tar_target(
    lh,
    create_location_history(
      input = c(input_1, input_2),
      # This changes based on data storage/format
      output_file = file.path(outdir, 'lh.parquet'),
      id_cols = c('source_system', 'source_id'),
      X = 'X',
      Y = 'Y'
    ),
    format = 'file'
  ),
  
  # Prepare ZIP code histories
  tar_target(
    zh,
    create_history_variable(
    input = c(input_1,input_2),
    output_file = file.path(outdir, 'zh.parquet'),
    id_cols = c('source_system', 'source_id'),
    variable = 'zip_code',
    clean_function = clean_zip_code
    ),
    format = 'file'
  )
)
```

#### Prepare frequency variables

`freqs` is a series of data frames saved as `.parquet` files that contain scaled relative frequency by value within the variables (e.g. `dob_clean`) specified in the column argument for [`create_frequency_table`](R/create_frequency_table.R).

```{targets context-variables}
list(
  # Create frequency tables
  tar_target(
    freqs,
    create_frequency_table(
      tables = list(data),
      columns = c('first_name_noblank', 'last_name_noblank', 'dob_clean'),
      output_folder = outdir
    ), format = 'file'
  )
)
```

### General Commentary

#### Types of identifiers

For this workflow, identifiers and other variables come in a few main flavors:

1.  Primary identifiers are things like source system, source id, name, date of birth, social security, and sex/gender. These variables generally do not vary over time – or at least, if they do, it happens only sporadically. These identifiers (or some similar subset) represent the core of a record and are often the basis for blocking criteria.
2.  Secondary identifiers are things like address, ZIP code, and phone number. These are things that may change with regularity and where the temporal characteristics of their appearance may be informative.
3.  Contextual variables are all other things that may be used for determining whether record A represents the same entity as record B. Some examples may include: household size, term frequency adjustment, possibility of being a twin, etc.

#### init_data()

The [`init_data`](R/init_data.R) function loads the data passed via the `input` argument and uses some hyrule functions to do common cleaning routines (in a somewhat opinionated manner). In this case, the two input datasets (`input_1` and `input_2`) are appended together and processed at the same time because the rest of the process assumes one "data" file/table with a `source_system` column that differentiates between the data specified by `input_1` versus `input_2`.

Within the `init_data` function, the following steps occur:

1.  Via [`hyrule::clean_names`](../R/utilities.R), name columns are stripped of non-character values and common prefixes/suffixes are removed. [`hyrule::remove_spaces`](../R/utilities.R) then removes any white space characters.
2.  Date of birth is converted into a date format and restricted to be from 1901 to the current year.
3.  Sex is converted into a standard character column ( e.g. 'Male' and 'Female' is converted to 'M' and 'F'). Anything not in those two categories is set to NA.
4.  Some SSN cleaning code is available as comments. This code, if applied, converts SSNs to strings of only numerics, removes common junk SSNs, and converts things to a standardized 9 digit number (as a character column to allow for SSNs beginning with 0).
5.  Certain "junk" names are set to NA.
6.  Rows with too much missing information are dropped from the dataset
7.  Rows with partial data density are filled in using data from other rows within the same source id. For example, if rows 1, 2 and 3 all derive from the same source id, and rows 2 and 3 have the same middle initial value but row 1 is NA, that NA is overwritten by the value present in rows 2 and 3. In the event that 2 and 3 disagree, then row 1 remains blank for that variable.

While these steps are good starting points for any linkage project, they should not be considered immutable or complete. Each project will likely require its own data cleaning routines as data can be messy in nearly infinite ways.

### Data cleaning

[Documentation from the splink package](https://moj-analytical-services.github.io/splink/demos/tutorials/01_Prerequisites.html) describes some good general principles of data cleaning. Some additional items include:

1.  Common "junk" names (e.g. John Doe) should be removed. Reviewing the names with high frequency is generally a good way to find "junk" names.
2.  Limit date of birth values to reasonable limits (e.g. 1900 - current year).

#### `clean_hash`

The final part of the `init_data` function is the creation of a row-wise hash generated from the cleaned primary identifiers – in this case, source system, source id, name, dob, and sex. This hash will subsequently be used as the main unique row identifier and it is used to nest variations of primary variables within a given source system and source id combination.

Practically, using the hash (instead of source id) as the low-level identifier instead of source id allows for more data available by source id to be leveraged to improve the linkage process. For example, if source ID sometimes has Richard as the first name and sometimes as Rick, then both permutations can be used for blocking and evaluating possible links to identify more matches.

Handling the data in this way (row based) instead of aggregating thing into list-columns and using set operations also improves computational efficiency – especially for blocking.

#### Frequency variables

Frequency variables are useful to include within the modelling framework as they can adjust/downweight common names. For example, "John Smith" and "John Smith" likely is less informative than "Unique Name" and "Unique Name" because John Smith is more common. Using the [`create_frequency_table`](R/create_frequency_table.R) function, any variable can be the source of a frequency variable. For a given input value (e.g. John), the resulting relative frequency value computed and then scaled between 0 and 1.

#### Example "input" data

The table below shows a few rows of the `input_1` target.

```{r inputexp, echo = FALSE}
knitr::kable(head(load_target('input_1'))) 
```

#### Example "cleaned" data

The table below displays a few rows of the `data` target. Recall that the `data` target is a "cleaned" version of `input_1` and `input_2`.

```{r dataexp, echo = F}
knitr::kable(head(load_target('data'))) 
```

## Constructing Training Data

### Overview

This section combines combines manually labeled (non-)matches the cleaned data and variables from the Data Preparation section.

### Targets

### General Commentary

### Input(s)

1.  A set of manually labeled pairs of records. The labeling should note whether the two records in the pair are a match (1) or not (0).
2.  The outputs of the "Prepare data" collection of steps.

### Output(s)

### Creating (new) training data

Unlike probabilistic methods (e.g. splink), machine learning methods require training data to operate. When beginning a new project, a few options are available:

1.  Borrow from a previously fit ML model
2.  Borrow from a probabilistic model
3.  Use some deterministic rules to generate matches/non-matches
4.  Randomly sampled manually labeled pairs

Regardless of the approach, the goal is to have enough pairs to fit a draft model. The draft model can then be used to generate match scores that can inform the selection of additional pairs for manual labeling.

### Prepping training data for this workflow

The `pairs` dataset within `hyrule` is at the source id rather than the hash level. This next target/block of code converts from source id to hash id. Generally, this step will not be required as the training data will be natively at the hash level. Additionally, most users will have their training pairs stored in a database and/or as a set of loadable files (e.g. csv) that can be read in with `tarchetypes::tar_files_input`.

Minimally, labeled pairs (e.g. training data) should be stored in the following format:

```{r, echo = FALSE}
a = data.frame(id1 = 1:2, id2 = 3:4, pair = c(0,1))
knitr::kable(a)
```

`id1`, `id2`, and `pair` are all required columns. `pair` must be a numeric column with the values of 0 (no-match), 1 (match), or -1 (unknown). Additional columns can be added to the dataset, but they will be unused by the current implementation.

```{targets prep-traindat}
list(
  # Note: This step is mostly just so the example(s) can run
  tar_target(train_input,
             convert_sid_to_hid(
               pairs = hyrule::train, 
               arrow::read_parquet(data),
               output_file = file.path(outdir, 'training.parquet')
               ), format = 'file'
             )
  
  # usually something like the following is better
  #tarchetypes::tar_files_input(train_input,'FILEPATHS to training data goes here'))
)



```

### Specifying a formula

Like most modelling/regression/machine learning exercises, a formula must be specified. Something like `pair ~ varA + varB + varC` where `pair` refers to the column in the training data indicating whether two records are a match and `varA`, `varB`, and `varC` are variables that are likely predictive of the match-iness between two records. For this exercise, the following variables are of interest (and will be computed – see below):

1.  `dob_year_exact`: exact match of year of birth
2.  `dob_mdham`: hamming distance between month and day of birth
3.  `gender_agree`: gender explicitly matches
4.  `first_name_jw`: jaro-winkler distance of first names
5.  `last_name_jw`: jaro-winkler distance of last names
6.  `name_swap_jw`: jaro-winkler distance of names with first and last swapped
7.  `complete_name_dl`: daimaru-levenstein distance between the full names. Full name is either first + last or first + middle + last. The minimum distance of the two versions is used.
8.  `middle_initial_agree`: Explicit match of middle initial
9.  `last_in_last`: where either records' whole last name is contained in the other one
10. `first_name_freq`: Scaled frequency tabulation of first names
11. `last_name_freq`: Scaled frequency tabulation of last names
12. `zip_overlap`: Binary flag indicating zip code histories ever overlapped (not accounting for time)
13. `exact_location`: binary flag indicating address histories overlap within 3 meters (location only – not spatio-temporal).

```{targets formula, tar_globals = TRUE}
f = pair ~ dob_year_exact + dob_mdham + gender_agree +
  first_name_jw + last_name_jw + name_swap_jw +
  complete_name_dl + middle_initial_agree + last_in_last +
  first_name_freq + last_name_freq +
  zip_overlap + exact_location
  

```

### Creating the variables

To create the variables specified by the formula, this workflow uses a function called [`make_model_frame`](R/make_model_frame.R). `make_model_frame` takes a number of inputs (datasets, the training pairs, frequency tables, etc.) and computes the required columns from the data. The actual computation is handled in a temporary DuckDB database so that expressive (and relatively portable) sql can be used. DuckDB is also quite clever when working with parquet files and optimizing queries.

### Compile training data

#### Load and prepare training data

At this stage, the `train_input` step is a basic data.frame (technically a file path to a data.frame) with three columns: `id1`, `id2`, and `pair`. As described above, the first two columns specify pairs of records while the `pair` column is a binary flag indicating whether the two (hash) ids are a match (1) or not (0). To be usable for model fitting, the training pairs must be screened for duplicates, contradictions (e.g. the first wave of training data says A !=B while the second set says A = B) must be reconciled, and the ids must be checked for validity (e.g. the input data might have changed the hash for a particular row so that a given pair in the training data doesn't have a counterpart in the main data).

#### Computing prediction variables

Once the training pairs are prepared, the predictor variables must be created. In this workflow, the `make_model_frame` function is used to do this (usually nested within another function). This function takes in a few parquet file paths (and/or table specified by `DBI::Id()`) and generates a sql query that when executed in the the correct environment will return a data frame containing pair level variables.

For preparing the training data, [`compile_training_data`](R/compile_training_data.R) loads and standardizes labeled pairs, uses `make_model_frame` (with the previously created targets as inputs) to go from labeled pairs to "training data." Before saving the results, some sanity checks are performed to ensure that too many of the labeled pairs get dropped due to missing data or some other set of data gremlins.

```{targets training-data}

list(
  tar_target(
    train_test_data, 
    compile_training_data(
        input = train_input,
        output_file = file.path(outdir, 'train_and_test.parquet'),
        formula = f,
        data = data,
        loc_history = lh,
        zip_history = zh,
        freq_tab_first_name = freqs[1],
        freq_tab_last_name = freqs[2],
        freq_tab_dob = freqs[3]
      )
    )
  )

```

### Test/train split

The labeled data is split into two chunks datasets "train" and "test." The training dataset will inform the models that will be fit on the later sections while the "test" dataset is held out and will be used to generate out of sample fit metrics.

```{targets split-test-train}
list(
  tar_target(
    training_hash,
    rlang::hash(arrow::read_parquet(train_test_data))
  ),
  
  tar_target(test_train_split, 
             split_tt(hash = training_hash, 
                      training_data = train_test_data, 
                      fraction = .15, 
                      train_of = file.path(outdir, 'train.parquet'), 
                      test_of = file.path(outdir, 'test.parquet'))
             , format = 'file')
  
)
```

## Model Specification

The linkage model consists of three parts:

1.  A lasso logistic regression that quickly screens out "obvious" matches and non-matches
2.  A diverse series of models that are fit via cross-validation.
3.  An ensemble model fit on the cross-validated results of the models generated in step 2 that makes the final match score.

Step 1 is used for computation efficiency as logistic regressions produce match scores quickly and are good enough to produce ballpark estimates. Together, steps 2 and 3 are a "stacking" ensemble model approach where a diverse set of models are combined together to produce results that are better than a single model.

### Screening model

```{targets screener}
list(
  tar_target(screener, 
  command = fit_screening_model(test_train_split[2], bounds, f))
)
```

### Submodels

For an effective ensemble, a variety of model families are employed. This example uses tuned versions of support vector machines, random forests, and xgboost gradient boosted machines.

```{targets submodel_specs, tar_globals = TRUE}
svm = parsnip::svm_linear(
  mode = 'classification', 
  engine = 'kernlab', 
  cost = tune())
rf = parsnip::rand_forest(
  mode = 'classification',
  trees = tune(),
  mtry = tune())
xg = parsnip::set_engine(
  parsnip::boost_tree(
    mode = 'classification',
    tree_depth = tune(),
    mtry = tune(), 
    trees = tune(),
    learn_rate = tune()),
  'xgboost', 
  objective = 'binary:logistic')

s_params = tibble::tribble(~NAME, ~MODEL,
                           'svm', quote(svm)
                           ,'rf', quote(rf)
                           ,'xg', quote(xg)
)
s_params$TUNER = 'bayes'
s_params$nm = s_params$NAME

  

```

The stacking framework requires the submodels/child models to be fit with cross-validation (as a check against over-fitting). As such, the training dataset is subdivided into 5 approximately equal sized chunks.

```{targets submod-folds}
list(
  tar_target(tfolds, command = make_folds(test_train_split[2], screener))
)
```

The models undergo [bayesian hyperparameter tuning](https://tune.tidymodels.org/reference/tune_bayes.html) and the best performing models are considered for inclusion in the ensemble.

```{targets submod-fit}
submods = list(
  tarchetypes::tar_map(
    values = s_params,
    names = 'nm',
    tar_target(submod,
               fit_submodel(
                 train = test_train_split[2],
                 screener = screener,
                 folds = tfolds,
                 m = MODEL,
                 theform = f,
                 apply_screen = apply_screen
                )
              )
  )
)

```

### Stacked ensemble model

The submodels are ensembled together via a lasso regression. The output model object is also saved.

```{targets z-ensemble}
list(
tarchetypes::tar_combine(
    model,
    submods,
    command = create_stacked_model(
      !!!.x,
      mnames = s_params$nm,
      screener = screener,
      bounds = bounds
    )
  ),
  tar_target(model_path,
  {
    saveRDS(model, file.path(outdir, 'model.rds'))
    file.path(outdir, 'model.rds')
  },
  format = 'file')
)
```

## Blocking

Blocking creates the list of pairs to evaluate for match scores. In this implementation, a series of sql statements are evaluated and the pairs that fit one or more of those conditions are cached for evaluation via the linkage model.

### Specifying blocking rules with SQL

Blocking rules are specified as DuckDB friendly SQL statements and refer to a `l` and a `r` dataset. In this implementation, both `l` and `r` are the `data` target. The `splink` package[has a great write up on good blocking rules](https://moj-analytical-services.github.io/splink/demos/tutorials/03_Blocking.html) and the approach used in this example is heavily influenced by the splink package.

Three rules are instantiated below:

1.  Exact match on last name
2.  Exact match on DOB
3.  Fuzzy match on first name and exact match on year

For further evaluation, a pair of records must meet at least one of the conditions.

```{targets block-1}
blocking_rules = c(
  'l.last_name_noblank = r.last_name_noblank',
  'l.dob_clean = r.dob_clean',
  "jaro_winkler_similarity(l.first_name_noblank, r.first_name_noblank) >.7
   and datepart('year', l.dob_clean) = datepart('year', r.dob_clean)"
  )

list(
  tarchetypes::tar_group_by(qgrid, 
                            command = make_block_rules(rules = blocking_rules),
                            qid)
)

```

### Computing and compiling pairs for evaluation

Once the blocking rules have been properly formatted and prepared, they are then computed. The `deduplicate` argument in the [`make_block`](R/blocking.R) function (`blocks` target) governs whether records from the same source system are "allowed" to be included in the final set of blocked pairs. Records from the same source system with the same source id are automatically excluded from final list of evaluation pairs as they will be added as fixed links by a later step.

```{targets block-2}
list(
tar_target(blocks, make_block(q = qgrid, 
                              data = data, 
                              id_col = 'clean_hash',
                              deduplicate = FALSE,
                              output_folder = outdir),
           pattern = map(qgrid),
           format = 'file'),

tar_target(bids, compile_blocks(blocks = blocks, 
                                output_folder = outdir, 
                                chk_size = 10000), format = 'file'),
tar_target(bid_files, bids)
)
```

At the end of the blocking process, the `bid_files` target links to a series of data.frames (saved as parquet files) that consist of the pairs that be assessed for match probability.

## Predicting Match Scores

### Using the ensemble

For all blocked pairs, a match score is generated and those where the match probability is \>.05 are saved. As described above, pairs are first evaluated by the lasso screening models and those that are within the range specified by `bounds` (default: $$.01 - .99$$) are further evaluated by the overall ensemble.

```{targets z-predictions}
tar_target(preds,
        predict_links(
          pairs = bid_files,
          model = model_path,
          output_folder = outdir,
          data = data,
          loc_history = lh,
          zip_history = zh,
          freq_tab_first_name = freqs[1],
          freq_tab_last_name = freqs[2],
          freq_tab_dob = freqs[3]
        ), 
        pattern = map(bid_files), 
        iteration = 'list', 
        format = 'file')
```

### Fixed links

Depending on the linkage problem, there may be certain pairs of records that must be considered matches, regardless of the match score. In this example, records with the same source system and source id, but different hash ids are automatically considered matches (they are ignored during the blocking step, but added here). The [fixed_links](R/fixed_links.R) function adds those records to the results set as exact matches (i.e. match score of 1).

```{targets fixed-links}
tar_target(fixed, 
             fixed_links(
               data = data,
               model_path = model_path,
               fixed_vars = c('source_system', 'source_id'),
               id_col = 'clean_hash',
               output_file = file.path(outdir, 'fixed_links.parquet')
             ), format = 'file'
           )
```

## Evaluation and Cutoffs

The ensemble produces match scores on a scale of 0 to 1. However, whether or not two records are a "match" is a binary value and therefore the scores must be discretized. To find a good cutoff point, 3 rounds of 5-fold cross validation are used. For each fold, the entire ensemble is refit via [`cv_refit`](R/cv_refit.R)(although the hyperparameters are inherited from the main model) on 4/5ths of the training data, and the cutoff that optimizes accuracy is computed.

```{targets a-cutoffs, tar_globals = T}
i = 3
cutoff_df = data.table::data.table(iter = 1:i, name = as.character(1:i))

```

```{targets b-cutoffs}
cv_cutoffs = list(
  tarchetypes::tar_map(
    values = cutoff_df,
    names = 'name',
    tar_target(cv_co,
                 cv_refit(
                   train = test_train_split[2],
                   model_path = model_path,
                   apply_screen = apply_screen,
                   iteration = iter,
                   nfolds = 5,
                   lasso = T 
                 ))
  )
)
```

### Identifying the cutpoint and OOS statistics

The cross-validated results are collected and summarized via the [`identify_cutoff`](R/identify_cutoff.R) function during the `cutme` target. The results are analyzed to determine a cutpoint that maximizes accuracy. Out of sample fit statistics are also generated during this step.

```{targets z-cutoffs}
list(
tarchetypes::tar_combine(
    cutme,
    cv_cutoffs,
    command = identify_cutoff(
      !!!.x,
      test = test_train_split[1],
      mods = model_path
    )
  ),
  tar_target(cutme_path,
  {
    saveRDS(cutme, file.path(outdir, 'cutme.rds'))
    file.path(outdir, 'cutme.rds')
  },
  format = 'file')
)
```

The resulting `cutme` target contains three items:

1.  A cut-point at the value of maximum accuracy, as computed from the cross validation process (the `cv_co` set of products)
2.  Summarized cross-validation results (when `Iteration == 0` in the resulting data.frame) and the constituent parts
3.  Out of sample fit metrics.

Reviewing the results, especially the out of sample fit metrics will give a good indication on how the model is performing. An example table is reproduced below:

```{r oostab, echo = FALSE}
oos = try(tar_read(cutme)[[3]])
if(inherits(oos,'try-error')) oos = data.frame('cutme target not found. Try rerunning tar_make()')
knitr::kable(oos)
```

## From 1:1 links to networks

Once a set of links at the hash id level have been identified, they must be aggregated to the level of interest – in this case, source id. This is done by organizing all the individual 1:1 links together into a series of networks and aggregating those networks by source id. The [`compile_links`](R/compile_links.R) function does this two step aggregation process and computes some metrics at each scale (e.g. the hash id level and then the source id level).

```{targets compile-components}
tar_target(components, 
           compile_links(
             preds,
             fixed,
             data = data, 
             id_col = 'clean_hash',
             cutpoint = cutme$cutpoint,
             output_folder = outdir
           ), format = 'file')
```

The second item in the output of the `components` target is a summary file, that reports the density (# of connects/# of total possible connections) and size (number of nodes within the cluster). Columns may be prefixed with `s#`, where the number refers to the level (each increasing level represents a nested subcluster).

```{r netmet, echo = FALSE}
net = try(tar_read(components))
if(!inherits(net, 'try-error')){
  net = setDT(arrow::read_parquet(net[2]))
}else{
  net = data.frame('components target not found. Try rerunning tar_make()')
}
knitr::kable(head(net))
```

### Applying constraints

While this example does not have any network based constraints, certain uses cases may require the implementation of deterministic rules. For example, when linking death records to other types of administrative information, users may want to enforce a rule that only one death record may within a network/cluster of linkages (otherwise, it would imply the "person" represented by the collection of records died more than once). The implementation of those sorts of rules is probably best done as part of the `compile_links` function (or as a new subsequent step).

### Identity Table

The first part of results stored in `components` target is a file path to a parquet file containing the final results: a data frame that crosswalks between `clean_hash`, `source_id` & `source_system`, and `final_comp_id`. This latter variable is final entity identity. All source ids with the same `final_comp_id` can be considered as representing the same "person" (or equivalent).

As currently implemented, the `final_comp_id`s are not persistent between model versions.

```{r showcomps, echo = F}
comps = load_target('components')
knitr::kable(head(comps))
```

# Final Pipeline

```{r printmermaid, results = "asis", echo = FALSE}

#cat(c("```{mermaid}",tar_mermaid(T), "```"), sep = "\n")
```

# Odds and ends

## Glossary

1.  Source system (`source_system`): A dataset

2.  Source Id (`source_id`): The unique record identifier within a source system. Within a source system, all records with the same source id are considered a single entity.

3.  hash id (`clean_hash`): A custom generated unique identifier that is nested within a source system and source id that specifies a specific set of identifiers. For example, Dan will result in a different hash id than Daniel, even with all the other inputs held the same. The hash id serves as the base unit of analysis for the matching (that is, the matches are between hash ids). Things are later aggregated to the source system - source id level

### Running this pipeline

To use/run this pipeline, users must:

1.  Install the relevant packages:

    Packages to setup the pipeline -

    ```{r packages1, eval = FALSE}
    install.packages(c('remotes', 'tarchetypes', 'targets', 'glue', 'sf', 'data.table'))

    remotes::install_github('PHSKC-APDE/hyrule')
    ```

    Packages required for the pipeline to run -

    ```{r packages2, eval = FALSE}
    install.packages(c('data.table', 'stringr', 'arrow', 'duckdb', 'DBI', 'glue', 'stringr', 'sf', 'tidymodels', 'workflows', 'stacks', 'dplyr', 'ranger', 'xgboost', 'rlang', 'igraph'))
    ```

2.  Render `_targets.qmd` (this file). This will convert the target-markdown cells in this document into .R files (stored in [`_targets_r`](_targets_r/)). Intrepid users can use the rendered results (specifically the `_targets.R` file and the files stored in `_targets_r`) if they'd rather no longer bother with the .qmd approach.

3.  Run `tar_make()` to execute the pipeline.

4.  (Optional) Re-render `_targets.qmd` to populate the few sections that pull data from the pipeline
