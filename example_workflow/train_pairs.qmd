---
title: "Making Training Pairs"
format: gfm
editor: visual
---

## Starting fresh

Any good record linkage workflow requires manually labeled pairs of records as estimating metrics of model performance and deciding on a cutoff threshold requires manually evaluating pairs. Machine learning linkage pipelines have an additional use for these pairs: fitting the match score model(s).

When beginning a new project, manually labeled pairs for the input datasets are probably not available and need to be created. A good way to select pairs for manual evaluation is to use a model or metric that can serve as an approximate match score and select pairs across the score spectrum. These initial match scores can be estimated through a few (non-comprehensive) mechanisms:

1.  Use predictions from an existing model, either one from another project and/or one trained on fake data. As long as the data cleaning/variable creation approach is sufficiently similiar than the resulting match scores will provide a good first match score estimate. If you are lucky, then maybe that is all you need. The overall idea is to do some bootleg transfer learning.
2.  Use predictions from a probabilistic model like splink. Worst case, you can use the results to generate some training data (or maybe even use it as a predictor). Best case, the probabilistic model is good enough for your use case and you are done with the project early.
3.  Compute some distance metrics on key variables (e.g., jaro-winkler distance on first name and last name), scale them, and take an average. Assuming the valences are aligned, you'll have a good enough starting point.

The goal is to generate pairs across the match score spectrum to fit a first model; about 100 evaluated pairs is enough to get started. While you can get fancy with how you select the pairs for manual evaluation, random sampling within quantiles is usually good enough.

Note: This section assumes you have already completed a first draft of the blocking scheme and therefore have a universe of possible pairs to evaluate. When computing approximate match scores, you don't need to assess all the pairs nor do so in a totally random way. Mostly, you want to evaluate enough pairs to represent the spectrum of possible match scores (as opposed to the distribution of actual match scores).

### Using a previously fit model

To use a previously fit model that used the example workflow structure, you will need to make the following changes to the workflow:

1.  Make sure the variable creation and dataset cleaning functions are the same/similar.
2.  Remove/comment out the following targets:
    1.  `train_input`
    2.  `train_test_data`
    3.  `training_hash`
    4.  `test_train_split`
    5.  `screener`
    6.  `tfolds`
    7.  `submod_*`
    8.  `model`
    9.  `cv_co_*`
    10. `fixed`
    11. `cutme`
    12. `components`
    13. `cutme_path`
3.  Overwrite the `model_path` target to point to the previously fit model

With these changes, you should be able to run the workflow up to the `preds` target.

### An example of selecting initial pairs

```{r, message=FALSE, warning=FALSE}
library('targets')
library('data.table')

preds = tar_read(preds) |>
  lapply(arrow::read_parquet) |>
  data.table::rbindlist()

# Select 10 pairs from each quantile
cuts = seq(0, .8, .2)
n = 20
selections = lapply(cuts, function(s){
  a = preds[final >= s & final <s+.2]
  a[sample(seq_len(.N), n), .(id1, id2, final)]
})

selections = rbindlist(selections)

knitr::kable(head(selections))

hist(selections[, final])

```

## Using hyrule::matchmaker()

The `matchmaker` function

## Ideas for selecting training data pairs
