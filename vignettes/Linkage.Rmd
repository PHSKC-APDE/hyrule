---
title: "Linkage"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Linkage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# A sample linkage

This vignette will show how to use the functions available through the `hyrule` package to help conduct data linkages. We will be linking two versions of "fake" data generated by the [psuedopeople package](https://pseudopeople.readthedocs.io/en/latest/index.html) in Python.

## Load data

```{r setup}
library(hyrule)
library('data.table')
library('stringdist')

# Load the data
# keep only a the columns needed for this vignette
kcols = c('simulant_id', 'first_name', 'middle_initial', 'last_name', 'date_of_birth', 'sex')
d1 = hyrule::fake_one[, .SD, .SDcols = kcols]
d2 = hyrule::fake_two[, .SD, .SDcols = kcols]

```

```{r show-data, echo = F}
knitr::kable(head(d1))
```

## Clean data

```{r}
d1c = hyrule::prep_data_for_linkage(d1,
                                   first_name = 'first_name',
                                   last_name = 'last_name',
                                   middle_name = 'middle_name',
                                   dob = 'date_of_birth',
                                   zip = NULL) # omitted for now
                                   
d2c = hyrule::prep_data_for_linkage(d2,
                                   first_name = 'first_name',
                                   last_name = 'last_name',
                                   middle_name = 'middle_name',
                                   dob = 'date_of_birth',
                                   zip = NULL) # omitted for now

# Add back some relevant variables
d1c = cbind(d1[, .(simulant_id, sex)], d1c)
d2c = cbind(d2[, .(simulant_id, sex)], d2c)

# create an id field specific to each dataset
d1c[, id := .I]
d2c[, id := .I]

# subset the data bit to create test/train
train1 = d1c[, sample(id, 5000)]
train2 = d2c[, sample(id, 5000)]

d1train = d1c[id %in% train1]
d2train = d2c[id %in% train2]

             
# knitr::kable(head(d2c))
```

## Create training dataset

### Blocking

To be evaluated as a possible match, a pair must exactly match on at least one component of date of birth. To catch data errors, pairs that potentially have swapped month/days will be considered. So will instances where DOB is missing

```{r}
# This might be memory expensive in real use cases
# Since it'll be nrow(d1) * nrow(d2) rows
# so you'll probably want to chunk it up
train = CJ(id1 = d1train[, id], id2 = d2train[, id])
train = merge(train, d1c[, .(id1 = id, dob1 = dob)], by = 'id1')
train = merge(train, d2c[, .(id2 = id, dob2 = dob)], by = 'id2')

# create rows for the conditions
find_keepers = function(input){
  input[, samemonth := month(dob1) == month(dob2)]
  input[, sameday := mday(dob1) == mday(dob2)]
  input[, sameyear := year(dob1) == year(dob2)]
  input[, swap := month(dob1) == mday(dob2) | mday(dob1) == month(dob2)]
  input[, keep := (samemonth + sameday + sameyear + swap) >0]

  input[keep == T | is.na(keep), .(id1, id2)]
}
train = find_keepers(train)

print(nrow(train))
print(round(100 * nrow(train)/(nrow(d1train) * nrow(d2train))))

```

### Creating variables

```{r make-vars}

train = merge(train, d1train[, .(sex1 = sex, 
                                 fn1 = first_name_noblank,
                                 ln1 = last_name_noblank,
                                 dob1 = dob,
                                 id1 = id)], by = 'id1')
train = merge(train, d2train[, .(sex2 = sex, 
                                 fn2 = first_name_noblank,
                                 ln2 = last_name_noblank,
                                 dob2 = dob,
                                 id2 = id)], by = 'id2')
compute_variables = function(input){
  # Hamming distance
  ham = function(x,y) stringdist(as.character(x), as.character(y), 'hamming')
  
  # Hamming distance of DOB
  input[, dob_ham := ham(dob1, dob2)]
  input[, mis_dob := as.integer(is.na(dob_ham))]
  input[, mean_dob_ham := mean(dob_ham, na.rm= T)]
  input[mis_dob == 1, dob_ham := mean_dob_ham]
  
  # do the sex designations disagree
  input[, sex_disagree := as.integer(sex1 != sex2)]
  input[is.na(sex_disagree), sex_disagree := 0]
  
  # first name distances
  ## cosine bigram
  input[!is.na(fn1) & !is.na(fn2),
        fn_cos2 := stringdist(fn1, fn2,method = 'cosine',
                              q = ifelse(nchar(fn1) <2 | nchar(fn2) <2,1,2))]
  input[is.na(fn_cos2), fn_cos2 := 1]
  
  ## jaro-winkler
  input[!is.na(fn1) & !is.na(fn2),
        fn_jw := stringdist(fn1, fn2,method = 'jw', p = .1)]
  input[is.na(fn_jw), fn_jw := 1]
  
  
  # last name differences
  ## cosine bigram
  input[!is.na(ln1) & !is.na(ln2),
        ln_cos2 := stringdist(ln1, ln2,method = 'cosine',
                              q = ifelse(nchar(ln1) <2 | nchar(ln2) <2,1,2))]
  input[is.na(ln_cos2), ln_cos2 := 1]
  
  ## jaro-winkler
  input[!is.na(ln1) & !is.na(ln2),
        ln_jw := stringdist(ln1, ln2,method = 'jw', p = .1)]
  input[is.na(ln_jw), ln_jw := 1]
  
  # combined name trigram cosine
  input[, cn1 := paste0(fn1,ln1)]
  input[, cn2 := paste0(fn2, ln2)]
  input[!is.na(cn1) | !is.na(cn2), 
        cn_cos := stringdist(cn1, cn2, 'cosine', q = 3 )]
  input[is.na(cn_cos), cn_cos := 1]
  
  # flags for identical daymonth
  input[, daymonth := as.integer(mday(dob1) == mday(dob2) & month(dob1) == month(dob2))]
  input[is.na(daymonth), daymonth := 0]
  
  return(input)
}
train = compute_variables(train)

```

### Outcome variable

Ordinarily, a user would have to manually identify matches (or use some sort of probabilistic interface aid) for the training dataset. However, since the data are "fake", matches are known a-priori.

```{r train-match}

train = merge(train, d1train[, .(id1 = id, sid1 = simulant_id)], by = 'id1')
train = merge(train, d2train[, .(id2 = id, sid2 = simulant_id)], by = 'id2')
train[, ismatch := as.integer(sid1==sid2)]

```

## Test dataset

```{r}

d1test = d1c[!id %in% train1]
d2test = d2c[!id %in% train2]
test = CJ(id1 = d1test[, id], id2 = d2test[, id])
test = merge(test, d1c[, .(id1 = id, dob1 = dob)], by = 'id1')
test = merge(test, d2c[, .(id2 = id, dob2 = dob)], by = 'id2')
test = find_keepers(test)

test = merge(test, d1test[, .(sex1 = sex, 
                                 fn1 = first_name_noblank,
                                 ln1 = last_name_noblank,
                                 dob1 = dob,
                                 id1 = id)], by = 'id1')
test = merge(test, d2test[, .(sex2 = sex, 
                                 fn2 = first_name_noblank,
                                 ln2 = last_name_noblank,
                                 dob2 = dob,
                                 id2 = id)], by = 'id2')
test = compute_variables(test)

# "truth"
test = merge(test, d1test[, .(id1 = id, sid1 = simulant_id)], by = 'id1')
test = merge(test, d2test[, .(id2 = id, sid2 = simulant_id)], by = 'id2')
test[, ismatch := as.integer(sid1==sid2)]

```

## Model Fitting

### fastLink

```{r, warning=FALSE}
library('fastLink')

# Standard approach is block on year, fastlink on first,last, and dob parts
# block on year of birth
# try without year of birth blocking since there are not that many records
# borrowed from threadr package since fastLink is noisey as hell
quiet <- function(x) {
  sink(tempfile())
  on.exit(sink())
  invisible(force(x))
}


quiet(link <- fastLink(
    dfA = d1c,
    dfB = d2c,
    varnames = c('first_name_noblank', 'last_name_noblank', 
                 'dob_day', 'dob_month', 'dob_year'),
    stringdist.match = c('first_name_noblank', 'last_name_noblank'),
    partial.match = c('first_name_noblank', 'last_name_noblank'),
    dedupe.matches = TRUE,
    n.cores = 1
  ))


  
# matchy matchy
res = data.table(mid1 = d1c[link$matches$inds.a, id],
                 mid2= d2c[link$matches$inds.b, id])

res = merge(res, d1c[, .(si1 = simulant_id, id)], all.x = T, by.x = 'mid1', by.y = 'id')
res = merge(res, d2c[, .(si2 = simulant_id, id)], all.x = T, by.x = 'mid2', by.y = 'id')

# number of correct matches over all found matches
cm = round(100*nrow(res[si1 == si2])/nrow(res))

# all possible matched data
mdat = merge(
  d1c[, .(simulant_id, fn1 = first_name_noblank, 
          ln1 = last_name_noblank, dob1 = dob, id1 = id)],
  d2c[, .(simulant_id, fn2 = first_name_noblank, 
          ln2 = last_name_noblank, dob2 = dob, id2 = id)], by = 'simulant_id')

# number of correct matches over all "TRUE" matches
tm = round(100 * nrow(res[si1==si2])/ nrow(mdat))

# limited to test simulants
test2 = copy(test[, .(sid1, sid2, ismatch, id1, id2)])
test2 = merge(test2, res[, .(id1 =mid1, id2 = mid2, match = 1)], all.x = T, by = c('id1', 'id2'))
test2 = test2[ismatch == 1 | !is.na(match)]

```

fastLink found `r nrow(res)` matches out of a total (test and train) of `r nrow(mdat)` matches. Of the matches identified, `r cm`% of them were correct.

Within the matches in the test dataset, `r test2[is.na(match) & ismatch == 1,.N]` were missed.

### Simple glm

```{r model-fit}

# start with a logistic regression
m1 = glm(ismatch ~ dob_ham + mis_dob + sex_disagree + fn_cos2 + fn_jw + ln_cos2 + ln_jw + cn_cos + daymonth, data = train, family = 'binomial')

# initial predictions
p1 = predict(m1, test,type = 'response')
test[, p1 := p1]
test[, p1bin := round(p1)]

tsum = test[ismatch == 1 | p1bin == 1, .N, keyby = .(ismatch, p1bin)]
tsum = tsum[, .(`True Match` = ismatch, `Pred Match` = p1bin, N)]

knitr::kable(tsum)
```

### SVM

```{r}
library('e1071')

# The full SVM takes too long
# use the insample predictions from the GLM to remove some of the "easier"
# calls that the glm can make
train[, insamp_glm := predict(m1, train, type = 'response')]

# remove all the extremely low probability stuff (>= 1/1000)
train_hard = train[insamp_glm>=.0001 & insamp_glm <=.9999]

# default svm
m2 = svm(ismatch ~ dob_ham + mis_dob + sex_disagree + fn_cos2 + fn_jw + ln_cos2 + ln_jw + cn_cos + daymonth, data = train_hard, type ='C-classification')

# Predict the ones that the lm didn't get super easily
test[, p2bin := p1bin]
#p1>=.0001 & p1<=.9999
test[p1>=.0001 & p1<=.9999, p2bin := as.numeric(as.character(predict(m2, .SD))), .SDcols = names(coef(m1))[-1]]

tsum = test[ismatch == 1 | p2bin == 1, .N, keyby = .(ismatch, p2bin)]
tsum = tsum[, .(`True Match` = ismatch, `Pred Match` = p2bin, N)]

knitr::kable(tsum)
```

### randomForest

```{r, warning = FALSE}
library('ranger')

m3 = ranger(ismatch ~ dob_ham + mis_dob + sex_disagree + fn_cos2 + fn_jw + ln_cos2 + ln_jw + cn_cos + daymonth, data = train_hard, num.trees = 1000)


# Predict the ones that the lm didn't get super easily
test[, p3bin := p1bin]
# p1>=.0001 & p1<=.9999
test[p1>=.0001 & p1<=.9999, p3bin := round(predict(m3, .SD)$predictions), .SDcols = names(coef(m1))[-1]]

tsum = test[ismatch == 1 | p3bin == 1, .N, keyby = .(ismatch, p3bin)]
tsum = tsum[, .(`True Match` = ismatch, `Pred Match` = p3bin, N)]

knitr::kable(tsum)

```

### xgboost

```{r}
library('xgboost')
train_hard_mat = model.matrix(~ dob_ham + mis_dob + sex_disagree + fn_cos2 + fn_jw + ln_cos2 + ln_jw + cn_cos + daymonth, train_hard)
train_hard_mat = train_hard_mat[,-1]
m4 = xgboost(data = train_hard_mat, label = train_hard$ismatch,
             objective = "binary:logistic", nrounds = 1000, verbose = 0)

test_mat = model.matrix(~ dob_ham + mis_dob + sex_disagree + fn_cos2 + fn_jw + ln_cos2 + ln_jw + cn_cos + daymonth, test[p1>=.0001 & p1<=.9999])
test_mat = test_mat[,-1]

tpreds = predict(m4, test_mat)
test[, p4bin := p1bin]
test[p1>=.0001 & p1<=.9999, p4bin := round(tpreds)]

tsum = test[ismatch == 1 | p4bin == 1, .N, keyby = .(ismatch, p4bin)]
tsum = tsum[, .(`True Match` = ismatch, `Pred Match` = p4bin, N)]

knitr::kable(tsum)


```

### Summary

```{r}
r = test[, .N, keyby= .(ismatch, glm = p1bin, svm = p2bin, rf = p3bin, xgb = p4bin)]
r[, Nmods := glm + svm + rf + xgb]
knitr::kable(r)
```
