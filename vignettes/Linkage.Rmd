---
title: "Getting Started with Hyrule"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Linkage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## hyrule

`hyrule` is a package that contains a few routines to facilitate machine learning record linkage (MLRL) via ensemble model approach (stacking).

# A sample linkage

This vignette will show how to use the functions available through the `hyrule` package to help conduct data linkages. We will be linking two versions of "fake" data generated by the [psuedopeople package](https://pseudopeople.readthedocs.io/en/latest/index.html) in Python.

For some additional details, [review this presentation](Ensemble%20machine%20learning%20for%20record%20linkage.pdf). It provides an overview of using machine learning for record linkage as well as a case study, comparisons to probabilistic methods, advice, and more (some of which is duplicated here).

## Set up

### Load packages

```{r packs, warning=FALSE, message=FALSE}
library(hyrule)
library('data.table')
library('xgboost')
library('ranger')
library('e1071')

```

### Load data

The `hyrule` package comes with two datasets that we will be using to demonstrate record linkage. Each dataset comes in three parts: (1) common identifiers like name, date of birth, sex, ZIP; (2) phone number history/list; and (3) location history (e.g. geocoded addresses). In general, most linkage problems will have at least the first part of the dataset (although certain columns are optional). #2 and #3 are useful, but optional.

#### Common Identifiers

```{r}

# Load the data
# keep only a the columns needed for this vignette
kcols = c('simulant_id', 'first_name', 'middle_initial', 'last_name', 'date_of_birth', 'sex', 'zip')
d1 = hyrule::fake_one[, .SD, .SDcols = kcols]
d2 = hyrule::fake_two[, .SD, .SDcols = kcols]
```

```{r show-data}
knitr::kable(head(d1))
```

#### Phone Numbers

Phone numbers should be long on individual id (in this case, `simulant_id`) and phone number entry. This way one id can be associated with multiple numbers.

```{r show-ph}
ph1 = hyrule::phone_history_one
ph2 = hyrule::phone_history_two
knitr::kable(head(ph1))
```

#### Location history

Location history should be an `sf` object long on the id variable and location entry. That is, each id can be associated with more than one address. If using location history type information, make sure that the data are points and the CRSs have been synchronized ahead of time.

```{r show-ah}
lh1 = hyrule::location_history_one
lh2 = hyrule::location_history_two
knitr::kable(head(lh1))
```

## Clean data

### Common Identifiers

The `prep_data_for_linkage` function provides data cleaning routines for variables commonly used in data linkage. These include name, date of birth, ZIP code, sex, and social security number.

```{r}
clean_d1 = prep_data_for_linkage(d1, first_name = 'first_name',
                                 last_name = 'last_name',
                                 dob = 'date_of_birth',
                                 middle_name = 'middle_initial',
                                 zip = 'zip',
                                 sex = 'sex')
clean_d2 = prep_data_for_linkage(d2, first_name = 'first_name',
                                 last_name = 'last_name',
                                 dob = 'date_of_birth',
                                 middle_name = 'middle_initial',
                                 zip = 'zip',
                                 sex = 'sex')

# Add back some relevant variables
d1 = cbind(d1[, .(simulant_id)], clean_d1)
d2 = cbind(d2[, .(simulant_id)], clean_d2)

# Create a dataset specific ID
# usually you won't have your data already linked
d1[, id1 := .I]
d2[, id2 := .I]

```

### Notes

Often location history and phone numbers will need to be cleaned (and/or reformatted). At present that must be done using common data manipulation tools (e.g. data.table and dplyr).

## The machine learning linkage loop

Machine learning record linkage works in three main steps as listed.

1.  A training dataset is created by manually labeling pairs of records as a match or not a match
2.  A set of models are fit on the training dataset
3.  The fit models are applied (e.g. prediction) to all of the potential pairs

Once a has been complete, you can review the results and figure out if additional variables need to be added and/or if the training data needs to be augmented.

## Blocking: generating possible pairs

Blocking is the process in which a-priori conditions are used to limit the potential universe of possible pairs. For example, you might require that a pair of records have the same year of birth before evaluating whether or not they are a match based on the full set of available information. Blocking turns a large problem (everyone compared to everyone else) into a set of slightly smaller ones (compare everyone born in 1990 with everyone else born in 1990). You can block on multiple things sequentially (e.g. year of birth and then sex) and/or jointly (sex AND year of birth). Sequential blocking schemes require some reconciliation process at the end to remove duplicates and do other clean up (beyond the scope of this vignette). This might also occur if you have missingness in your blocking variable and you allow those rows to be assessed in multiple blocks.

### Example

In this example, we block on date of birth year. For simplicity, we are only going to work with records between 1900 and 2022.

```{r, pos-pairs}

posyears = unique(c(d1[, dob_year], d2[, dob_year]))
posyears = na.omit(posyears)
posyears = sort(posyears[posyears>=1900 & posyears<=2022])

# Create a list of lists
# Each entry is the ids from each dataset belonging to the block
# as determined by DOB year
blocks = lapply(posyears, function(x){
  list(a = d1[dob_year == x, id1], b = d2[dob_year == x, id2])
})



```

## Making a training dataset

## Transfer learning

In general, I recommend starting with a pre-trained model and using those models to make an initial pass at your data. If you are luckily, the previously trained model is good enough and your job is done. More likely, you'll want to use these initial predictions to inform how you select pairs to manually identify as a match or not. It is probably better to use a a model fit on "real" data (ask Daniel about what is available), but if that is not available, you can use `hyrule::generate_starter()` to create a model fit on fake data.

Note: This example will be a little weird since we are fitting a model on the same fake data that is standing in as the "real" data for this vignette. Usually this won't be a problem.
